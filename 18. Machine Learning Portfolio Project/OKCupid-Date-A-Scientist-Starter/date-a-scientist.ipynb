{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that we’ve never had before about how different people experience romance.\n",
    "\n",
    "In this portfolio project, you will analyze some data from OKCupid, an app that focuses on using multiple choice and short answers to match users.\n",
    "\n",
    "You will also create a presentation about your findings from this OKCupid dataset.\n",
    "\n",
    "The purpose of this project is to practice formulating questions and implementing machine learning techniques to answer those questions. However, the questions you ask and how you answer them are entirely up to you.\n",
    "\n",
    "We’re excited to see the different topics you explore.\n",
    "\n",
    "Project Objectives:\n",
    "- Complete a project to add to your portfolio\n",
    "- Use Jupyter Notebook to communicate findings\n",
    "- Build, train, and evaluate a machine learning model\n",
    "\n",
    "Prerequisites:\n",
    "- Natural Language Processing\n",
    "- Supervised Machine Learning\n",
    "- Unsupervised Machine Learning\n",
    "\n",
    "\n",
    "The dataset provided has the following columns of multiple-choice data:\n",
    "\n",
    "- body_type\n",
    "- diet\n",
    "- drinks\n",
    "- drugs\n",
    "- education\n",
    "- ethnicity\n",
    "- height\n",
    "- income\n",
    "- job\n",
    "- offspring\n",
    "- orientation\n",
    "- pets\n",
    "- religion\n",
    "- sex\n",
    "- sign\n",
    "- smokes\n",
    "- speaks\n",
    "- status\n",
    "\n",
    "And a set of open short-answer responses to :\n",
    "\n",
    "- essay0 - My self summary\n",
    "- essay1 - What I’m doing with my life\n",
    "- essay2 - I’m really good at\n",
    "- essay3 - The first thing people usually notice about me\n",
    "- essay4 - Favorite books, movies, show, music, and food\n",
    "- essay5 - The six things I could never do without\n",
    "- essay6 - I spend a lot of time thinking about\n",
    "- essay7 - On a typical Friday night I am\n",
    "- essay8 - The most private thing I am willing to admit\n",
    "- essay9 - You should message me if…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('',axis=0,inplace=True)\n",
    "df.rename(columns={'essay0': 'my_self', 'essay1': 'life', 'essay2': 'good_at', 'essay3': 'people_notice', \n",
    "                         'essay4': 'favorites', 'essay5': 'six_needed', 'essay6': 'think_to', 'essay7': 'friday_night', \n",
    "                         'essay8': 'private_admit', 'essay9': 'message_me_if'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a list with essay columns names\n",
    "essays_cols = df.columns.to_list()[6:16]\n",
    "\n",
    "# Remove newlines and HTML charachters from essay columns\n",
    "for col in essays_cols:\n",
    "    df[col] = df[col].str.replace(\"\\n\", \" \", regex=False)\n",
    "    df[col] = df[col].str.replace(r\"<[^>]*>\", \"\", regex=True)\n",
    "    df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    df[col] = df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all the essay questions are populated for every user I am going to consolidate them all into one column called Essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay'] = df[df.columns[6:16]].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "df['essay'] = df['essay'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frequent_words(data):\n",
    "    cnt = Counter()\n",
    "    for text in data.values:\n",
    "        for word in text.split(' '):\n",
    "            cnt[word] += 1\n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "    data = data.apply(\n",
    "        lambda text: \" \".join([word for word in str(text).split(' ') if word not in FREQWORDS]))\n",
    "    print('remove_frequent_words applied')\n",
    "    return data\n",
    "\n",
    "df['essay'] = remove_frequent_words(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(df.essay[0])\n",
    "doc5 = nlp(df.essay[1])\n",
    "doc6 = nlp(df.essay[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8857345011788965\n",
      "0.7192977675925847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrend\\AppData\\Local\\Temp/ipykernel_13276/2238249604.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc4.similarity(doc5))\n",
      "C:\\Users\\mrend\\AppData\\Local\\Temp/ipykernel_13276/2238249604.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc4.similarity(doc6))\n"
     ]
    }
   ],
   "source": [
    "print(doc4.similarity(doc5))\n",
    "print(doc4.similarity(doc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mmmmmm idk what say so just ask me hair manga black tie laptop someone funny ill comeback dont have anything else do really'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.essay[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_frequent_words applied\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Essay Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = data.apply(\n",
    "        lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    print('lemmatize_words applied')\n",
    "    # print(data_frame[column_name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize_words applied\n"
     ]
    }
   ],
   "source": [
    "df['essay'] = lemmatize_words(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    number_pattern = r'\\d+'\n",
    "    data = data.apply(\n",
    "        lambda text: re.sub(pattern=number_pattern, repl=\" \", string=text))\n",
    "    print('remove_numbers applied')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_numbers applied\n"
     ]
    }
   ],
   "source": [
    "df['essay'] = remove_numbers(df['essay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out how much of a match someone is based on their response to my_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
