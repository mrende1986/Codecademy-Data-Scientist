{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that we’ve never had before about how different people experience romance.\n",
    "\n",
    "In this portfolio project, you will analyze some data from OKCupid, an app that focuses on using multiple choice and short answers to match users.\n",
    "\n",
    "You will also create a presentation about your findings from this OKCupid dataset.\n",
    "\n",
    "The purpose of this project is to practice formulating questions and implementing machine learning techniques to answer those questions. However, the questions you ask and how you answer them are entirely up to you.\n",
    "\n",
    "We’re excited to see the different topics you explore.\n",
    "\n",
    "Project Objectives:\n",
    "- Complete a project to add to your portfolio\n",
    "- Use Jupyter Notebook to communicate findings\n",
    "- Build, train, and evaluate a machine learning model\n",
    "\n",
    "Prerequisites:\n",
    "- Natural Language Processing\n",
    "- Supervised Machine Learning\n",
    "- Unsupervised Machine Learning\n",
    "\n",
    "\n",
    "The dataset provided has the following columns of multiple-choice data:\n",
    "\n",
    "- body_type\n",
    "- diet\n",
    "- drinks\n",
    "- drugs\n",
    "- education\n",
    "- ethnicity\n",
    "- height\n",
    "- income\n",
    "- job\n",
    "- offspring\n",
    "- orientation\n",
    "- pets\n",
    "- religion\n",
    "- sex\n",
    "- sign\n",
    "- smokes\n",
    "- speaks\n",
    "- status\n",
    "\n",
    "And a set of open short-answer responses to :\n",
    "\n",
    "- essay0 - My self summary\n",
    "- essay1 - What I’m doing with my life\n",
    "- essay2 - I’m really good at\n",
    "- essay3 - The first thing people usually notice about me\n",
    "- essay4 - Favorite books, movies, show, music, and food\n",
    "- essay5 - The six things I could never do without\n",
    "- essay6 - I spend a lot of time thinking about\n",
    "- essay7 - On a typical Friday night I am\n",
    "- essay8 - The most private thing I am willing to admit\n",
    "- essay9 - You should message me if…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('',axis=0,inplace=True)\n",
    "df.rename(columns={'essay0': 'my_self', 'essay1': 'life', 'essay2': 'good_at', 'essay3': 'people_notice', \n",
    "                         'essay4': 'favorites', 'essay5': 'six_needed', 'essay6': 'think_to', 'essay7': 'friday_night', \n",
    "                         'essay8': 'private_admit', 'essay9': 'message_me_if'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all the essay questions are populated for every user I am going to consolidate them all into one column called Essay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to combine all essay columns into one. No longer used\n",
    "df['essay'] = df[df.columns[6:16]].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "df['essay'] = df['essay'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_prep(data):\n",
    "    \n",
    "    data = data.str.replace(\"\\n\", \" \", regex=False)\n",
    "    data = data.str.replace(r\"<[^>]*>\", \"\", regex=True)\n",
    "    data = data.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    data = data.str.lower()\n",
    "    print('regex applied')\n",
    "    \n",
    "    def remove_numbers(data):\n",
    "        number_pattern = r'\\d+'\n",
    "        data = data.apply(\n",
    "            lambda text: re.sub(pattern=number_pattern, repl=\" \", string=text))\n",
    "        print('remove_numbers applied')\n",
    "        return data\n",
    "    \n",
    "    data = remove_numbers(data)\n",
    "    \n",
    "    def remove_frequent_words(data):\n",
    "        cnt = Counter()\n",
    "        for text in data.values:\n",
    "            for word in text.split(' '):\n",
    "                cnt[word] += 1\n",
    "        FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "        data = data.apply(\n",
    "            lambda text: \" \".join([word for word in str(text).split(' ') if word not in FREQWORDS]))\n",
    "        print('remove_frequent_words applied')\n",
    "        return data\n",
    "\n",
    "    data = remove_frequent_words(data)\n",
    "    \n",
    "    def lemmatize_words(data):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        data = data.apply(\n",
    "            lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "        print('lemmatize_words applied')\n",
    "        # print(data_frame[column_name])\n",
    "        return data\n",
    "\n",
    "    data = lemmatize_words(data)\n",
    "    \n",
    "    data = [nlp(data[x]) for x in range(len(data))]\n",
    "    print('nlp data applied')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my_self']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['life']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['good_at']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['people_notice']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['favorites']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['six_needed']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['think_to']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['friday_night']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['private_admit']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n",
      "['message_me_if']\n",
      "regex applied\n",
      "remove_numbers applied\n",
      "remove_frequent_words applied\n",
      "lemmatize_words applied\n",
      "nlp data applied\n"
     ]
    }
   ],
   "source": [
    "essays_cols = df.columns.to_list()[6:16]\n",
    "\n",
    "for col in essays_cols:\n",
    "    print([col])\n",
    "    df[col] = essay_prep(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrend\\AppData\\Local\\Temp/ipykernel_10760/2416845102.py:5: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  temp_list.append(data[userid].similarity(data[i]))\n"
     ]
    }
   ],
   "source": [
    "userid = 0\n",
    "temp_list = []\n",
    "def userid_v_others(data):\n",
    "    for i in range(len(data)):\n",
    "        temp_list.append(data[userid].similarity(data[i]))\n",
    "    return temp_list\n",
    "\n",
    "for col in essays_cols:\n",
    "    temp_list = []\n",
    "    df[str(col)+'_score'] = userid_v_others(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out how much of a match someone is based on their response to essay questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "designing thing using my hand thinking creative solution problem but im not so good identifying problem coping with chaos"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.good_at.iloc[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people laugh ranting about good salting finding simplicity complexity complexity simplicity"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.good_at.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>my_self</th>\n",
       "      <th>life</th>\n",
       "      <th>good_at</th>\n",
       "      <th>people_notice</th>\n",
       "      <th>favorites</th>\n",
       "      <th>six_needed</th>\n",
       "      <th>think_to</th>\n",
       "      <th>friday_night</th>\n",
       "      <th>private_admit</th>\n",
       "      <th>message_me_if</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>height</th>\n",
       "      <th>income</th>\n",
       "      <th>job</th>\n",
       "      <th>last_online</th>\n",
       "      <th>location</th>\n",
       "      <th>offspring</th>\n",
       "      <th>orientation</th>\n",
       "      <th>pets</th>\n",
       "      <th>religion</th>\n",
       "      <th>sex</th>\n",
       "      <th>sign</th>\n",
       "      <th>smokes</th>\n",
       "      <th>speaks</th>\n",
       "      <th>status</th>\n",
       "      <th>my_self_score</th>\n",
       "      <th>life_score</th>\n",
       "      <th>good_at_score</th>\n",
       "      <th>people_notice_score</th>\n",
       "      <th>favorites_score</th>\n",
       "      <th>six_needed_score</th>\n",
       "      <th>think_to_score</th>\n",
       "      <th>friday_night_score</th>\n",
       "      <th>private_admit_score</th>\n",
       "      <th>message_me_if_score</th>\n",
       "      <th>essay_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>(about, me, would, love, think, that, wa, some...</td>\n",
       "      <td>(currently, working, a, an, international, age...</td>\n",
       "      <td>(people, laugh, ranting, about, good, salting,...</td>\n",
       "      <td>(way, look, am, six, foot, half, asian, half, ...</td>\n",
       "      <td>(book, absurdistan, republic, mouse, men, only...</td>\n",
       "      <td>(food, water, cell, phone, shelter)</td>\n",
       "      <td>(duality, humorous, thing)</td>\n",
       "      <td>(trying, to, find, someone, to, hang, am, down...</td>\n",
       "      <td>(am, new, california, looking, for, someone, w...</td>\n",
       "      <td>(want, be, swept, off, your, foot, tired, norm...</td>\n",
       "      <td>asian, white</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>transportation</td>\n",
       "      <td>2012-06-28-20-30</td>\n",
       "      <td>south san francisco, california</td>\n",
       "      <td>doesn&amp;rsquo;t have kids, but might want them</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism and very serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>gemini</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>40</td>\n",
       "      <td>fit</td>\n",
       "      <td></td>\n",
       "      <td>socially</td>\n",
       "      <td></td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>(do, nt, really, like, summarizing, myself, bu...</td>\n",
       "      <td>(spending, lot, of, time, building, thing, bus...</td>\n",
       "      <td>(designing, thing, using, my, hand, thinking, ...</td>\n",
       "      <td>(i, ve, never, figured, out, answer, this, que...</td>\n",
       "      <td>(book, almost, anything, by, vonnegut, or, ste...</td>\n",
       "      <td>(ignoring, obvious, air, water, food, shelter,...</td>\n",
       "      <td>(build, thingsmake, thing, better, cosmos, our...</td>\n",
       "      <td>(there, is, no, typical, night, for, me, somet...</td>\n",
       "      <td>(have, no, secret, wo, nt, tell, but, have, as...</td>\n",
       "      <td>(you, re, still, reading, at, least, say, hell...</td>\n",
       "      <td>white</td>\n",
       "      <td>71.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>construction / craftsmanship</td>\n",
       "      <td>2012-06-30-00-01</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>doesn&amp;rsquo;t have kids</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs</td>\n",
       "      <td>agnosticism but not too serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>gemini and it&amp;rsquo;s fun to think about</td>\n",
       "      <td>no</td>\n",
       "      <td>english (okay), french (poorly), spanish (poor...</td>\n",
       "      <td>single</td>\n",
       "      <td>0.976987</td>\n",
       "      <td>0.921732</td>\n",
       "      <td>0.793368</td>\n",
       "      <td>0.904595</td>\n",
       "      <td>0.895263</td>\n",
       "      <td>0.764673</td>\n",
       "      <td>0.610774</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>0.773201</td>\n",
       "      <td>0.930465</td>\n",
       "      <td>0.844891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>31</td>\n",
       "      <td>average</td>\n",
       "      <td></td>\n",
       "      <td>socially</td>\n",
       "      <td></td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>(when, it, come, own, life, do, a, please, but...</td>\n",
       "      <td>(read, book, soak, up, a, much, sun, a, humanl...</td>\n",
       "      <td>(conversation, analyzing, movie, spelling, gra...</td>\n",
       "      <td>(people, generally, ca, nt, discern, ethnicity...</td>\n",
       "      <td>(short, list, awakening, intelligence, one, hu...</td>\n",
       "      <td>(bicycle, cell, phone, laptop, food, water, ob...</td>\n",
       "      <td>(finding, balance, all, aspect, life, also, co...</td>\n",
       "      <td>(jumble, in, between, extreme, ton, at, root, ...</td>\n",
       "      <td>(list, dirty, dancing, a, one, deep, down, gui...</td>\n",
       "      <td>(like, bike, riding, you, re, swimmer, that, s...</td>\n",
       "      <td></td>\n",
       "      <td>71.0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>2012-06-05-13-04</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td></td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism and somewhat serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>libra but it doesn&amp;rsquo;t matter</td>\n",
       "      <td>when drinking</td>\n",
       "      <td>english (fluently), spanish (poorly)</td>\n",
       "      <td>single</td>\n",
       "      <td>0.930046</td>\n",
       "      <td>0.915309</td>\n",
       "      <td>0.753655</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.875889</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.557309</td>\n",
       "      <td>0.915803</td>\n",
       "      <td>0.781329</td>\n",
       "      <td>0.921246</td>\n",
       "      <td>0.840847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>30</td>\n",
       "      <td>average</td>\n",
       "      <td></td>\n",
       "      <td>often</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>(am, new, san, francisco, bay, area, looking, ...</td>\n",
       "      <td>(write, software, fun, profit, complain, about...</td>\n",
       "      <td>(like, think, am, good, communicating, friend,...</td>\n",
       "      <td>(wear, funny, tshirts, they, have, funny, ando...</td>\n",
       "      <td>(book, almost, anything, fantasy, lord, ring, ...</td>\n",
       "      <td>(happiness, fun, hug, fresh, air, internet, i,...</td>\n",
       "      <td>(life, universe, everything, me, friend, every...</td>\n",
       "      <td>(doing, same, thing, do, every, night, in, cas...</td>\n",
       "      <td>(hmm, so, want, know, little, secret, ey, well...</td>\n",
       "      <td>(should, message, me, ifwell, feel, like, it, ...</td>\n",
       "      <td></td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>computer / hardware / software</td>\n",
       "      <td>2012-06-29-22-56</td>\n",
       "      <td>menlo park, california</td>\n",
       "      <td>doesn&amp;rsquo;t have kids</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes cats</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>m</td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), dutch (fluently), lisp (fl...</td>\n",
       "      <td>single</td>\n",
       "      <td>0.975224</td>\n",
       "      <td>0.891383</td>\n",
       "      <td>0.789386</td>\n",
       "      <td>0.917571</td>\n",
       "      <td>0.903603</td>\n",
       "      <td>0.629294</td>\n",
       "      <td>0.617737</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.805243</td>\n",
       "      <td>0.912192</td>\n",
       "      <td>0.835753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33</td>\n",
       "      <td>fit</td>\n",
       "      <td></td>\n",
       "      <td>socially</td>\n",
       "      <td></td>\n",
       "      <td>working on masters program</td>\n",
       "      <td>(just, moved, bay, area, from, austin, tx, ori...</td>\n",
       "      <td>(making, music, programming, getting, back, in...</td>\n",
       "      <td>(i, m, from, louisiana, so, cooking, eating, a...</td>\n",
       "      <td>(lately, keep, getting, asked, are, you, with,...</td>\n",
       "      <td>(moviestvetc, big, lebowski, other, cohen, bro...</td>\n",
       "      <td>(in, no, particular, order, food, music, outdo...</td>\n",
       "      <td>(methodology, for, practicing, creative, skill...</td>\n",
       "      <td>(just, moved, here, am, still, getting, to, kn...</td>\n",
       "      <td>(am, in, s, still, can, not, grow, mustache, i...</td>\n",
       "      <td>(want, help, me, assemble, ikea, stuff, andor,...</td>\n",
       "      <td>white</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>entertainment / media</td>\n",
       "      <td>2012-06-29-16-08</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td></td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td></td>\n",
       "      <td>m</td>\n",
       "      <td>pisces but it doesn&amp;rsquo;t matter</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english (fluently), c++ (fluently), german (po...</td>\n",
       "      <td>single</td>\n",
       "      <td>0.975612</td>\n",
       "      <td>0.877780</td>\n",
       "      <td>0.740459</td>\n",
       "      <td>0.906484</td>\n",
       "      <td>0.853540</td>\n",
       "      <td>0.725052</td>\n",
       "      <td>0.608091</td>\n",
       "      <td>0.938251</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.913045</td>\n",
       "      <td>0.834105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age       body_type               diet    drinks  drugs  \\\n",
       "0    22  a little extra  strictly anything  socially  never   \n",
       "43   40             fit                     socially          \n",
       "59   31         average                     socially          \n",
       "40   30         average                        often  never   \n",
       "16   33             fit                     socially          \n",
       "\n",
       "                            education  \\\n",
       "0       working on college/university   \n",
       "43  graduated from college/university   \n",
       "59  graduated from college/university   \n",
       "40     graduated from masters program   \n",
       "16         working on masters program   \n",
       "\n",
       "                                              my_self  \\\n",
       "0   (about, me, would, love, think, that, wa, some...   \n",
       "43  (do, nt, really, like, summarizing, myself, bu...   \n",
       "59  (when, it, come, own, life, do, a, please, but...   \n",
       "40  (am, new, san, francisco, bay, area, looking, ...   \n",
       "16  (just, moved, bay, area, from, austin, tx, ori...   \n",
       "\n",
       "                                                 life  \\\n",
       "0   (currently, working, a, an, international, age...   \n",
       "43  (spending, lot, of, time, building, thing, bus...   \n",
       "59  (read, book, soak, up, a, much, sun, a, humanl...   \n",
       "40  (write, software, fun, profit, complain, about...   \n",
       "16  (making, music, programming, getting, back, in...   \n",
       "\n",
       "                                              good_at  \\\n",
       "0   (people, laugh, ranting, about, good, salting,...   \n",
       "43  (designing, thing, using, my, hand, thinking, ...   \n",
       "59  (conversation, analyzing, movie, spelling, gra...   \n",
       "40  (like, think, am, good, communicating, friend,...   \n",
       "16  (i, m, from, louisiana, so, cooking, eating, a...   \n",
       "\n",
       "                                        people_notice  \\\n",
       "0   (way, look, am, six, foot, half, asian, half, ...   \n",
       "43  (i, ve, never, figured, out, answer, this, que...   \n",
       "59  (people, generally, ca, nt, discern, ethnicity...   \n",
       "40  (wear, funny, tshirts, they, have, funny, ando...   \n",
       "16  (lately, keep, getting, asked, are, you, with,...   \n",
       "\n",
       "                                            favorites  \\\n",
       "0   (book, absurdistan, republic, mouse, men, only...   \n",
       "43  (book, almost, anything, by, vonnegut, or, ste...   \n",
       "59  (short, list, awakening, intelligence, one, hu...   \n",
       "40  (book, almost, anything, fantasy, lord, ring, ...   \n",
       "16  (moviestvetc, big, lebowski, other, cohen, bro...   \n",
       "\n",
       "                                           six_needed  \\\n",
       "0                 (food, water, cell, phone, shelter)   \n",
       "43  (ignoring, obvious, air, water, food, shelter,...   \n",
       "59  (bicycle, cell, phone, laptop, food, water, ob...   \n",
       "40  (happiness, fun, hug, fresh, air, internet, i,...   \n",
       "16  (in, no, particular, order, food, music, outdo...   \n",
       "\n",
       "                                             think_to  \\\n",
       "0                          (duality, humorous, thing)   \n",
       "43  (build, thingsmake, thing, better, cosmos, our...   \n",
       "59  (finding, balance, all, aspect, life, also, co...   \n",
       "40  (life, universe, everything, me, friend, every...   \n",
       "16  (methodology, for, practicing, creative, skill...   \n",
       "\n",
       "                                         friday_night  \\\n",
       "0   (trying, to, find, someone, to, hang, am, down...   \n",
       "43  (there, is, no, typical, night, for, me, somet...   \n",
       "59  (jumble, in, between, extreme, ton, at, root, ...   \n",
       "40  (doing, same, thing, do, every, night, in, cas...   \n",
       "16  (just, moved, here, am, still, getting, to, kn...   \n",
       "\n",
       "                                        private_admit  \\\n",
       "0   (am, new, california, looking, for, someone, w...   \n",
       "43  (have, no, secret, wo, nt, tell, but, have, as...   \n",
       "59  (list, dirty, dancing, a, one, deep, down, gui...   \n",
       "40  (hmm, so, want, know, little, secret, ey, well...   \n",
       "16  (am, in, s, still, can, not, grow, mustache, i...   \n",
       "\n",
       "                                        message_me_if     ethnicity  height  \\\n",
       "0   (want, be, swept, off, your, foot, tired, norm...  asian, white    75.0   \n",
       "43  (you, re, still, reading, at, least, say, hell...         white    71.0   \n",
       "59  (like, bike, riding, you, re, swimmer, that, s...                  71.0   \n",
       "40  (should, message, me, ifwell, feel, like, it, ...                  76.0   \n",
       "16  (want, help, me, assemble, ikea, stuff, andor,...         white    70.0   \n",
       "\n",
       "    income                             job       last_online  \\\n",
       "0       -1                  transportation  2012-06-28-20-30   \n",
       "43   60000    construction / craftsmanship  2012-06-30-00-01   \n",
       "59      -1                                  2012-06-05-13-04   \n",
       "40      -1  computer / hardware / software  2012-06-29-22-56   \n",
       "16      -1           entertainment / media  2012-06-29-16-08   \n",
       "\n",
       "                           location  \\\n",
       "0   south san francisco, california   \n",
       "43        san francisco, california   \n",
       "59        san francisco, california   \n",
       "40           menlo park, california   \n",
       "16              oakland, california   \n",
       "\n",
       "                                       offspring orientation  \\\n",
       "0   doesn&rsquo;t have kids, but might want them    straight   \n",
       "43                       doesn&rsquo;t have kids    straight   \n",
       "59                                                  straight   \n",
       "40                       doesn&rsquo;t have kids    straight   \n",
       "16                                                  straight   \n",
       "\n",
       "                         pets                                   religion sex  \\\n",
       "0   likes dogs and likes cats      agnosticism and very serious about it   m   \n",
       "43                 likes dogs   agnosticism but not too serious about it   m   \n",
       "59  likes dogs and likes cats  agnosticism and somewhat serious about it   m   \n",
       "40                 likes cats                                agnosticism   m   \n",
       "16  likes dogs and likes cats                                              m   \n",
       "\n",
       "                                        sign         smokes  \\\n",
       "0                                     gemini      sometimes   \n",
       "43  gemini and it&rsquo;s fun to think about             no   \n",
       "59         libra but it doesn&rsquo;t matter  when drinking   \n",
       "40                                                       no   \n",
       "16        pisces but it doesn&rsquo;t matter      sometimes   \n",
       "\n",
       "                                               speaks  status  my_self_score  \\\n",
       "0                                             english  single       1.000000   \n",
       "43  english (okay), french (poorly), spanish (poor...  single       0.976987   \n",
       "59               english (fluently), spanish (poorly)  single       0.930046   \n",
       "40  english (fluently), dutch (fluently), lisp (fl...  single       0.975224   \n",
       "16  english (fluently), c++ (fluently), german (po...  single       0.975612   \n",
       "\n",
       "    life_score  good_at_score  people_notice_score  favorites_score  \\\n",
       "0     1.000000       1.000000             1.000000         1.000000   \n",
       "43    0.921732       0.793368             0.904595         0.895263   \n",
       "59    0.915309       0.753655             0.886667         0.875889   \n",
       "40    0.891383       0.789386             0.917571         0.903603   \n",
       "16    0.877780       0.740459             0.906484         0.853540   \n",
       "\n",
       "    six_needed_score  think_to_score  friday_night_score  private_admit_score  \\\n",
       "0           1.000000        1.000000            1.000000             1.000000   \n",
       "43          0.764673        0.610774            0.877851             0.773201   \n",
       "59          0.871213        0.557309            0.915803             0.781329   \n",
       "40          0.629294        0.617737            0.915900             0.805243   \n",
       "16          0.725052        0.608091            0.938251             0.802734   \n",
       "\n",
       "    message_me_if_score  essay_match  \n",
       "0              1.000000     1.000000  \n",
       "43             0.930465     0.844891  \n",
       "59             0.921246     0.840847  \n",
       "40             0.912192     0.835753  \n",
       "16             0.913045     0.834105  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Average Score of matched essays\n",
    "df['essay_match'] = df[df.columns[-10:]].mean(axis=1)\n",
    "\n",
    "# Sort top 10 highest match\n",
    "df.sort_values(['essay_match'],ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
